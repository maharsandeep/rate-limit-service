# ğŸš€ Rate Limit Service

A scalable and high-performance **Rate Limit Service** built using the
**Token Bucket Algorithm** with in-memory caching.

This service provides per-user rate limiting for Spring Boot
applications and is designed to handle **high concurrency**, support
**horizontal scaling**, and maintain **efficient memory usage** across
different deployment environments.

------------------------------------------------------------------------

## ğŸ“Œ Key Features

-   ğŸ”¢ **Token Bucket Algorithm** (burst-aware rate limiting)
-   ğŸ§  **Concurrency-safe implementation** (thread-safe bucket updates)
-   ğŸ’¾ **Efficient Memory Management** using Caffeine cache
-   ğŸ”Œ **Pluggable Cache Layer** (in-memory by default, Redis-supported
    for distributed systems)
-   ğŸ“ˆ **Horizontally Scalable Architecture**
-   ğŸ³ **Docker Containerized** and deployment-ready

------------------------------------------------------------------------

## ğŸ— Architecture

### High-Level Flow

``` mermaid
flowchart LR
    A[Client] --> B[Controller]
    B --> C[Handler]
    C --> D[Cache Layer]
    D --> E[User Bucket]
```

------------------------------------------------------------------------

### Component Description

### 1ï¸âƒ£ Client

External system or user making HTTP API requests to the rate limit
service.

### 2ï¸âƒ£ Controller

-   Entry point for REST APIs\
-   Validates requests\
-   Routes traffic to the handler layer

### 3ï¸âƒ£ Handler

-   Implements the core rate-limiting logic\
-   Applies Token Bucket algorithm\
-   Ensures thread-safe execution for concurrent requests

### 4ï¸âƒ£ Cache Layer

-   Stores client buckets\
-   Default: In-memory (Caffeine)\
-   Optional: Redis or other distributed cache provider

### 5ï¸âƒ£ User Bucket

-   Maintains token state per client\
-   Tracks refill rate and consumption\
-   Determines whether a request is allowed or rejected

------------------------------------------------------------------------

### ğŸ”„ Request Lifecycle

1.  Client sends request\
2.  Controller receives and forwards to handler\
3.  Handler checks cache for client bucket\
4.  Bucket tokens are consumed if available\
5.  Response returned (Allowed / Rate Limit Exceeded)

------------------------------------------------------------------------

### ğŸ“ˆ Scalability Notes
The caching layer can operate in:
-   **In-Memory Mode (Default)** supports single-instance deployments
-   **Distributed Cache Mode**  enables horizontal scaling across multiple
    service instances
-   Designed for high concurrency with efficient memory management


------------------------------------------------------------------------

## ğŸ“¦ Technologies Used

| Technology      | Purpose                     | Version |
|---------------|----------------------------|----------|
| Java          | Programming Language        | 17 |
| Spring Boot   | Application Framework       | 4.0.3 |
| Caffeine      | In-memory Caching           | Spring Boot managed version |
| Maven         | Build Tool                  | â€” |
| Docker        | Containerization            | â€” |
| Redis (Optional) | Distributed Cache Backend | Latest compatible |

------------------------------------------------------------------------

## ğŸ§  Concurrency & Performance

### âœ… Concurrency Handling

-   Thread-safe bucket access to prevent race conditions
-   Safe handling of concurrent requests for the same client
-   Designed for high-throughput API environments

### ğŸ’¾ Efficient Memory Usage

-   Uses Caffeine in-memory cache for ultra-fast lookups
-   Automatic eviction and expiration
-   Minimal memory footprint for large client sets

### ğŸ“ˆ Scalability

| Mode | Description |
|------|------------|
| Single Node | Runs fully in-memory with zero external dependencies |
| Distributed | Plug in Redis (or any cache provider) to share rate-limit state across multiple service instances |

------------------------------------------------------------------------

## ğŸ³ Run Locally (Docker)

### 1ï¸âƒ£ Build the Project

```bash
# Build the project (skip tests if needed)
mvn clean install -DskipTests
```

### 2ï¸âƒ£ Build Docker Image

```bash
docker build -t ratelimitservice .`
```
### 3ï¸âƒ£ Run Container

```bash
docker run -d -p 8080:8080 --name ratelimitservice-container
ratelimitservice`
```
### 4ï¸âƒ£ Verify Health

`curl --location 'localhost:8080/health'`

Expected Output:

`RUNNING`

------------------------------------------------------------------------

## ğŸ” How to Test

### 1ï¸âƒ£ Access Before Registration
```bash
curl --location 'localhost:8080/limit/api/v1/allow/test-client-01'
```

Response:

`{ "allowed": false, "error": "Client not found or not registered. Please
register first." }`

------------------------------------------------------------------------

### 2ï¸âƒ£ Register a Client
```bash
curl --location 'localhost:8080/limit/api/v1/client'\
--header 'Content-Type: application/json'\
--data '{ "clientId" : "test-client-01", "limitPerMinute" : 5 }'
```

**Note**: We have set 5 requests are allowed in 1 min

------------------------------------------------------------------------

### 3ï¸âƒ£ Call Rate Limit Endpoint
```bash
curl --location 'localhost:8080/limit/api/v1/allow/test-client-01'
```

For first 5 requests within a minute, the response is:

`{ "allowed": true }`

------------------------------------------------------------------------

### 4ï¸âƒ£ Sixth Request Within Same Minute

The response is:

`{ "allowed": false, "error": "Rate limit exceeded. Please try after some
time." }`

------------------------------------------------------------------------

### 5ï¸âƒ£ After 1 Minute

Once the token bucket refills (after 1 minute), requests will be allowed
again for 5 times.

------------------------------------------------------------------------

## ğŸ” Cache Modes

### ğŸ§  In-Memory Mode (Default)

-   Uses Caffeine cache
-   Ideal for single-instance deployment
-   Extremely low latency
-   Automatic cleanup & expiry

### ğŸŒ Distributed Mode (Redis or Any Cache Provider)

-   Shared rate-limit state across multiple service instances
-   Enables horizontal scaling
-   Suitable for Kubernetes or clustered deployments
-   Cache abstraction layer allows plugging in Redis easily

------------------------------------------------------------------------

## ğŸš¢ Containerization & Deployment

* Fully containerized with Docker
* Ready for:
  * Kubernetes
  * Docker Swarm
  * ECS
  * Any container orchestration platform
* Works seamlessly with external Redis containers for distributed
    scaling

------------------------------------------------------------------------

## ğŸ§ª Test Strategy

-   Manual testing via cURL commands (as shown above)
-   Spring Boot unit and integration tests
-   Concurrency testing by simulating parallel requests
-   Distributed cache testing by integrating Redis backend

------------------------------------------------------------------------

## ğŸ“˜ Summary

This Rate Limit Service is:

-   âš¡ High-performance
-   ğŸ”’ Concurrency-safe
-   ğŸ’¾ Memory-efficient
-   ğŸ“ˆ Horizontally scalable
-   ğŸ³ Container-ready
-   ğŸ”Œ Pluggable with Redis or other cache providers

------------------------------------------------------------------------

### â­ Ready for Production Deployment

Designed to run efficiently on a single machine or scale seamlessly
across distributed systems.
